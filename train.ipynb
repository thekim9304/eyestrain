{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import utils.deeplabv3_ as build_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import utils.draw_predict as dp\n",
    "import utils.call_data_v2 as cd\n",
    "\n",
    "class_num = 2\n",
    "learning_rate = 0.001\n",
    "total_epoch = 200\n",
    "batch_size = 1\n",
    "\n",
    "img_path = '../dataset/trn_img/'\n",
    "label_path = '../dataset/trn_labelmask/'\n",
    "\n",
    "img_list = glob.glob(img_path + '*')\n",
    "label_list = glob.glob(label_path + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print('Error: Creating directory. ' + directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1122 23:46:10.459161 31808 deprecation_wrapper.py:119] From C:\\Users\\th_k9\\Desktop\\pupil_segmentation\\tensorflow_version\\utils\\deeplabv3_.py:154: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1122 23:46:10.462153 31808 deprecation_wrapper.py:119] From C:\\Users\\th_k9\\Desktop\\pupil_segmentation\\tensorflow_version\\utils\\deeplabv3_.py:155: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1122 23:46:10.463150 31808 deprecation.py:506] From C:\\Users\\th_k9\\AppData\\Local\\Continuum\\anaconda3\\envs\\Kimtae\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1122 23:46:10.485090 31808 deprecation_wrapper.py:119] From C:\\Users\\th_k9\\Desktop\\pupil_segmentation\\tensorflow_version\\utils\\deeplabv3_.py:168: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1122 23:46:10.501048 31808 deprecation.py:323] From C:\\Users\\th_k9\\Desktop\\pupil_segmentation\\tensorflow_version\\utils\\deeplabv3_.py:113: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W1122 23:46:12.661156 31808 deprecation_wrapper.py:119] From C:\\Users\\th_k9\\Desktop\\pupil_segmentation\\tensorflow_version\\utils\\deeplabv3_.py:29: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 [=====================] cost : 0.1415537\n",
      "Epoch : 2 [=====================] cost : 0.01251217\n",
      "Epoch : 3 [=====================] cost : 0.004315174\n",
      "Epoch : 4 [=====================] cost : 0.003304996\n",
      "Epoch : 5 [=====================] cost : 0.003148833\n",
      "Epoch : 6 [=====================] cost : 0.003105559\n",
      "Epoch : 7 [=====================] cost : 0.003089407\n",
      "Epoch : 8 [=====================] cost : 0.003076536\n",
      "Epoch : 9 [=====================] cost : 0.003066645\n",
      "Epoch : 10 [=====================] cost : 0.003056207\n",
      "Epoch : 11 [=====================] cost : 0.003054712\n",
      "Epoch : 12 [=====================] cost : 0.003049331\n",
      "Epoch : 13 [=====================] cost : 0.003040515\n",
      "Epoch : 14 [=====================] cost : 0.003035629\n",
      "Epoch : 15 [=====================] cost : 0.003031173\n",
      "Epoch : 16 [=====================] cost : 0.003026612\n",
      "Epoch : 17 [=====================] cost : 0.003023161\n",
      "Epoch : 18 [=====================] cost : 0.003019623\n",
      "Epoch : 19 [=====================] cost : 0.003019016\n",
      "Epoch : 20 [=====================] cost : 0.003010894\n",
      "Epoch : 21 [=====================] cost : 0.003005855\n",
      "Epoch : 22 [=====================] cost : 0.003004297\n",
      "Epoch : 23 [=====================] cost : 0.00299725\n",
      "Epoch : 24 [=====================] cost : 0.002992653\n",
      "Epoch : 25 [=====================] cost : 0.002992447\n",
      "Epoch : 26 [=====================] cost : 0.002984918\n",
      "Epoch : 27 [=====================] cost : 0.002979288\n",
      "Epoch : 28 [=====================] cost : 0.002974964\n",
      "Epoch : 29 [=====================] cost : 0.002968613\n",
      "Epoch : 30 [=====================] cost : 0.002963797\n",
      "Epoch : 31 [=====================] cost : 0.002957046\n",
      "Epoch : 32 [=====================] cost : 0.002951116\n",
      "Epoch : 33 [=====================] cost : 0.002954167\n",
      "Epoch : 34 [=====================] cost : 0.002944797\n",
      "Epoch : 35 [=====================] cost : 0.002934402\n",
      "Epoch : 36 [=====================] cost : 0.002928018\n",
      "Epoch : 37 [=====================] cost : 0.00292093\n",
      "Epoch : 38 [=====================] cost : 0.002913085\n",
      "Epoch : 39 [=====================] cost : 0.002907587\n",
      "Epoch : 40 [=====================] cost : 0.002902314\n",
      "Epoch : 41 [=====================] cost : 0.002896935\n",
      "Epoch : 42 [=====================] cost : 0.002885446\n",
      "Epoch : 43 [=====================] cost : 0.002267872\n",
      "Epoch : 44 [=====================] cost : 0.001788113\n",
      "Epoch : 45 [=====================] cost : 0.001789405\n",
      "Epoch : 46 [=====================] cost : 0.001775547\n",
      "Epoch : 47 [=====================] cost : 0.001768321\n",
      "Epoch : 48 [=====================] cost : 0.001760157\n",
      "Epoch : 49 [=====================] cost : 0.001751963\n",
      "Epoch : 50 [=====================] cost : 0.001746023\n",
      "Epoch : 51 [=====================] cost : 0.001739072\n",
      "Epoch : 52 [=====================] cost : 0.001731608\n",
      "Epoch : 53 [=====================] cost : 0.001717051\n",
      "Epoch : 54 [=====================] cost : 0.0002798351\n",
      "Epoch : 55 [=====================] cost : 0.0002688203\n",
      "Epoch : 56 [=====================] cost : 0.0002631169\n",
      "Epoch : 57 [=====================] cost : 0.0002572109\n",
      "Epoch : 58 [=====================] cost : 0.0002527163\n",
      "Epoch : 59 [=====================] cost : 0.0002465829\n",
      "Epoch : 60 [=====================] cost : 0.0002426151\n",
      "Epoch : 61 [=====================] cost : 0.0002383908\n",
      "Epoch : 62 [=====================] cost : 0.0002321497\n",
      "Epoch : 63 [=====================] cost : 0.0002302442\n",
      "Epoch : 64 [=====================] cost : 0.0002219059\n",
      "Epoch : 65 [=====================] cost : 0.0002221953\n",
      "Epoch : 66 [=====================] cost : 0.0002160102\n",
      "Epoch : 67 [=====================] cost : 0.0002110536\n",
      "Epoch : 68 [=====================] cost : 0.0002054035\n",
      "Epoch : 69 [=====================] cost : 0.0002059949\n",
      "Epoch : 70 [=====================] cost : 0.0001997681\n",
      "Epoch : 71 [=====================] cost : 0.0001954222\n",
      "Epoch : 72 [=====================] cost : 0.0001930328\n",
      "Epoch : 73 [=====================] cost : 0.0001896987\n",
      "Epoch : 74 [=====================] cost : 0.0001862472\n",
      "Epoch : 75 [=====================] cost : 0.0001846396\n",
      "Epoch : 76 [=====================] cost : 0.0001817\n",
      "Epoch : 77 [=====================] cost : 0.0001772592\n",
      "Epoch : 78 [=====================] cost : 0.0001759437\n",
      "Epoch : 79 [=====================] cost : 0.0001735112\n",
      "Epoch : 80 [=====================] cost : 0.0001711507\n",
      "Epoch : 81 [=====================] cost : 0.0001758913\n",
      "Epoch : 82 [=====================] cost : 0.0001644793\n",
      "Epoch : 83 [=====================] cost : 0.0001646611\n",
      "Epoch : 84 [=====================] cost : 0.0001616129\n",
      "Epoch : 85 [=====================] cost : 0.0001586411\n",
      "Epoch : 86 [=====================] cost : 0.0001577681\n",
      "Epoch : 87 [=====================] cost : 0.0001554886\n",
      "Epoch : 88 [=====================] cost : 0.000154023\n",
      "Epoch : 89 [=====================] cost : 0.0001508903\n",
      "Epoch : 90 [=====================] cost : 0.0001497453\n",
      "Epoch : 91 [=====================] cost : 0.0001479752\n",
      "Epoch : 92 [=====================] cost : 0.0001480399\n",
      "Epoch : 93 [=====================] cost : 0.0001443586\n",
      "Epoch : 94 [=====================] cost : 0.0001429278\n",
      "Epoch : 95 [=====================] cost : 0.0001407031\n",
      "Epoch : 96 [=====================] cost : 0.0001393899\n",
      "Epoch : 97 [=====================] cost : 0.0001374423\n",
      "Epoch : 98 [=====================] cost : 0.0001360146\n",
      "Epoch : 99 [=====================] cost : 0.0001353404\n",
      "Epoch : 100 [=====================] cost : 0.000133131\n",
      "Epoch : 101 [=====================] cost : 0.0001317965\n",
      "Epoch : 102 [=====================] cost : 0.0001295552\n",
      "Epoch : 103 [=====================] cost : 0.0001283477\n",
      "Epoch : 104 [=====================] cost : 0.0001272577\n",
      "Epoch : 105 [=====================] cost : 0.0001255831\n",
      "Epoch : 106 [=====================] cost : 0.0001249494\n",
      "Epoch : 107 [=====================] cost : 0.0001254899\n",
      "Epoch : 108 [=====================] cost : 0.0001215021\n",
      "Epoch : 109 [=====================] cost : 0.000123094\n",
      "Epoch : 110 [=====================] cost : 0.0001190752\n",
      "Epoch : 111 [=====================] cost : 0.0001189856\n",
      "Epoch : 112 [=====================] cost : 0.0001183866\n",
      "Epoch : 113 [=====================] cost : 0.0001163124\n",
      "Epoch : 114 [=====================] cost : 0.0001170463\n",
      "Epoch : 115 [=====================] cost : 0.0001146399\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n",
    "Y = tf.placeholder(tf.int32, shape=[None, None, None])  \n",
    "\n",
    "test_img = cv2.imread('../dataset/trn_img/0_0.jpg')\n",
    "test_img = test_img.reshape([1, test_img.shape[0], test_img.shape[1], test_img.shape[2]])\n",
    "\n",
    "model = build_model.deeplabv3(X, class_num, True)\n",
    "predict = tf.argmax(model, 3)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = model, labels = Y))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_data = len(img_list)\n",
    "\n",
    "random.seed(444)\n",
    "random_indexes = random.sample(range(0, total_data), total_data)\n",
    "total_batch = int(total_data / batch_size)\n",
    "\n",
    "for i in range(total_epoch):\n",
    "    print('Epoch : {} ['.format(i + 1), end='')\n",
    "\n",
    "    total_cost = 0\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for batch_idx in range(total_batch):\n",
    "        trn_x, trn_y = cd.read_batch(batch_idx, batch_size, img_list, label_list, random_indexes)\n",
    "        \n",
    "        _, c = sess.run([train_op, cost], feed_dict = {X : trn_x, Y : trn_y})\n",
    "        \n",
    "        total_cost += c\n",
    "        \n",
    "        if batch_idx % int(total_batch / 20) == 0:\n",
    "            print('=', end='')\n",
    "            \n",
    "    avg_cost = total_cost / (total_batch + 1)\n",
    "\n",
    "    print('] cost : {0:.7}'.format(avg_cost))\n",
    "\n",
    "    p = sess.run([predict], feed_dict = {X : test_img})\n",
    "    segmentation = dp.draw_pixel(p[0][0], class_num)\n",
    "    cv2.imwrite('../dataset/trn_test/' + str(i) + '.png', segmentation)\n",
    "\n",
    "    createFolder('../dataset/model/' + str(i))\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, '../dataset/model/' + str(i) + '/' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dataset/model/total_model'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../dataset/model/total_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kimtae",
   "language": "python",
   "name": "kimtae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
